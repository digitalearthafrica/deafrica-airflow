import boto3
import json
import gzip
import csv
import os.path as op
from copy import deepcopy
from datetime import datetime, timedelta
from airflow.contrib.hooks.aws_hook import AwsHook

from urllib.parse import urlparse


class s3:
    def __init__(self, url, conn, region, suffix=""):
        self.url = url
        self.suffix = suffix
        aws_hook = AwsHook(aws_conn_id=conn)
        cred = aws_hook.get_session().get_credentials()
        self.s3 = boto3.client(
            "s3",
            aws_access_key_id=cred.access_key,
            aws_secret_access_key=cred.secret_key,
            region_name=region,
        )
        self.bucket = self.urlparse(self.url)["bucket"]

    # Modified derived from https://alexwlchan.net/2018/01/listing-s3-keys-redux/
    def find(self, url, sub_key):
        """
        Generate objects in an S3 bucket.
        :param sub_key:
        :param: sub_key: string to be present in the object name
        :param url: The URL of the bucket
        """

        parts = self.urlparse(url)
        kwargs = {"Bucket": parts["bucket"]}

        while True:
            # The S3 API response is a large blob of metadata.
            # 'Contents' contains information about the listed objects.
            resp = self.s3.list_objects_v2(**kwargs)
            try:
                contents = resp["Contents"]
            except KeyError:
                return

            for obj in contents:
                key = obj["Key"]
                if sub_key in key and key.endswith(self.suffix):
                    yield f"s3://{parts['bucket']}/{obj['Key']}"

            # The S3 API is paginated, returning up to 1000 keys at a time.
            # Pass the continuation token into the next response, until we
            # reach the final page (when this field is missing).
            try:
                kwargs["ContinuationToken"] = resp["NextContinuationToken"]
            except KeyError:
                break

    def latest_manifest(self):
        """
        Return a dictionary of a manifest file"
        """
        parts = self.urlparse(self.url)
        # get latest manifest file
        today = datetime.now()
        manifest_url = None
        for dt in [today, today - timedelta(1)]:
            _key = op.join(parts["key"], dt.strftime("%Y-%m-%d"))
            _url = f"s3://{parts['bucket']}"
            manifests = [k for k in self.find(_url, _key)]
            if len(manifests) == 1:
                manifest_url = manifests[0]
                parts = self.urlparse(manifest_url)
                break
        if manifest_url:
            print("Manifest file:", manifest_url)
            s3_clientobj = self.s3.get_object(Bucket=parts["bucket"], Key=parts["key"])
            return json.loads(s3_clientobj["Body"].read().decode("utf-8"))
        else:
            return None

    def https_to_s3(self, url):
        """ Convert https s3 URL to an s3 URL """
        parts = urlparse(url)
        bucket = parts.netloc.split(".")[0]
        s3url = f"s3://{bucket}{parts.path}"
        return s3url

    def urlparse(self, url):
        """ Split S3 URL into bucket, key, filename """
        _url = deepcopy(url)
        if url[0:5] == "https":
            _url = self.https_to_s3(url)
        if _url[0:5] != "s3://":
            raise Exception("Invalid S3 url %s" % _url)

        url_obj = _url.replace("s3://", "").split("/")

        # remove empty items
        url_obj = list(filter(lambda x: x, url_obj))
        return {"bucket": url_obj[0], "key": "/".join(url_obj[1:])}

    def list_keys(self):
        manifest = self.latest_manifest()
        for obj in manifest["files"]:
            print(obj["key"])
            gzip_obj = self.s3.get_object(Bucket=self.bucket, Key=obj["key"])
            buffer = gzip.open(gzip_obj["Body"], mode="rt")
            reader = csv.reader(buffer)
            for row in reader:
                yield row
