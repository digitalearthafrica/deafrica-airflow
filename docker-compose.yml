version: '3.7'
services:
  postgres:
    image: postgres:12
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - pgdata:/var/lib/postgresql/data/pgdata
    logging:
      options:
        max-size: 10m
        max-file: "3"
  setup-airflow:
    image: &image apache/airflow:1.10.14
    depends_on:
      - postgres
    environment: &env
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres/airflow
      # Uncomment and add fernet key to encrypt/decrypt secrets. For details see
      # https://airflow.readthedocs.io/en/stable/howto/secure-connections.html
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__SMTP__SMTP_HOST=maildev
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=False
      - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__WEBSERVER__RBAC=True
#      - AIRFLOW__CORE__LOGGING_LEVEL=DEBUG
# I think we eventually want to switch store_dag_code and store_serialized_dags on.
# They're likely to become the defaults in the future.
# See https://airflow.apache.org/docs/stable/dag-serialization.html
#      - AIRFLOW__CORE__STORE_DAG_CODE=True
#      - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
      # Default Connections Used in the DEA Airflow deployment
      - AIRFLOW_CONN_LPGS_GADI=ssh://dra547@gadi.nci.org.au/
      - AIRFLOW_CONN_DEA_PUBLIC_DATA_UPLOAD=s3://foo:bar@dea-public-data-dev/
      - AIRFLOW_CONN_AWS_NCI_DB_BACKUP=s3://foo:bar@dea-db-backups/
      - HOST_USER_ID=${UID}
      - HOST_GROUP_ID=${GID}
    entrypoint: /bin/bash
     # The webserver initializes permissions, so sleep for that to (approximately) be finished
     # No disaster if the webserver isn't finished by then, but create_user will start spitting out errors until the permissions exist
    command: -c 'airflow initdb && sleep 5 && airflow create_user --role Admin --username airflow --password airflow -e airflow@airflow.com -f airflow -l airflow'

  webserver:
    image: *image
    restart: always
    depends_on:
      - postgres
      - setup-airflow
    environment: *env
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes: &volumes
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - logs/:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt
      - /dev/urandom:/dev/random   # Required to get non-blocking entropy source
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
  scheduler:
    image: *image
    restart: always
    depends_on:
      - postgres
      - setup-airflow
    environment: *env
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes: *volumes
    command: scheduler
    ports:
      - "8793:8793"
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-scheduler.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
  maildev:
    image: maildev/maildev
    ports:
      - "1080:80"
volumes:
  pgdata:
  logs:
